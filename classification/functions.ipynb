{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions for text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf\n",
    "import sys\n",
    "import scipy.sparse\n",
    "import sklearn.metrics\n",
    "import sklearn.naive_bayes\n",
    "import sklearn.model_selection\n",
    "#import keras\n",
    "import nltk\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_context('notebook') \n",
    "sns.set_style('ticks')\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (7, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seed = 5\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run \"tqdm.ipynb\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tok_text(text):\n",
    "    tok = nltk.word_tokenize(text)\n",
    "    tok_no_punctuation = [word.lower() for word in tok if word.isalpha()]\n",
    "    tok_no_stopwords = [word for word in tok_no_punctuation if word not in stopwords]\n",
    "    tokens = [nltk.stem.porter.PorterStemmer().stem(word) for word in tok_no_stopwords]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count the total frequency of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_freq(data, column_name):\n",
    "    dic = nltk.FreqDist()\n",
    "    for words in data[column_name]:\n",
    "        for word in words:\n",
    "                dic[word] += 1\n",
    "                \n",
    "    return dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count the relative frequency of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_freq_class(data, n_words, column_name, ref_value, col_feature):\n",
    "    n_class = len(data[data[column_name]==ref_value])\n",
    "    fdist = nltk.FreqDist()\n",
    "    for words in data[data[column_name]==ref_value][col_feature]:\n",
    "        for word in np.unique(words):\n",
    "            fdist[word] += 1\n",
    "            \n",
    "    common = pd.Series(dict(fdist))/n_class\n",
    "    common= common.sort_values(ascending=False)\n",
    "    finalList = common.head(n_words).round(3)\n",
    "    \n",
    "    return n_class, finalList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_freq_class_total(data, n_words, column_name, ref_value, col_feature):\n",
    "    fdist = nltk.FreqDist()\n",
    "    for words in data[data[column_name]==ref_value][col_feature]:\n",
    "        for word in np.unique(words):\n",
    "            fdist[word] += 1\n",
    "    common = pd.Series(dict(fdist))/n_class\n",
    "    common= common.sort_values(ascending=False)\n",
    "    finalList = common.head(n_words).round(3)\n",
    "    \n",
    "    return n_class, finalList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the dataset in train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_train_test(data, frac):\n",
    "    #data[clas]=(data['Class']==clas).astype(int)\n",
    "    train = data.sample(frac=frac, random_state=5)\n",
    "    test = data[data.index.isin(train.index)==False].copy()\n",
    "    train = train.reset_index(drop=True)\n",
    "    test = test.reset_index(drop=True)\n",
    "    print('Classification on train dataset: ' , train['Class_n'].value_counts(normalize = True))\n",
    "    print('Classification on test dataset: ', test['Class_n'].value_counts(normalize = True))\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-entropy to rank the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def training_error(feature, data, y_train, col_feature):\n",
    "    X_train = design_matrix_one(feature, data[col_feature])\n",
    "    nbc = sklearn.naive_bayes.BernoulliNB()\n",
    "    model = nbc.fit(X_train, np.ravel(y_train))\n",
    "    prob = model.predict_proba(X_train)\n",
    "    return sklearn.metrics.log_loss(y_train, prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# given one feature\n",
    "def design_matrix_one(feature, series):\n",
    "    X = series.apply(lambda text_tok: (feature in text_tok))\n",
    "    X = X.astype(int) \n",
    "    return X.values.reshape((-1,1)) # converting to a NumPy matrix, as required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# given a list of features\n",
    "def design_matrix(features, series):\n",
    "    X = scipy.sparse.lil_matrix((len(series),len(features)))\n",
    "    for i in range(len(series)):\n",
    "        tokens = series.iloc[i]\n",
    "        for j, feature in enumerate(features):\n",
    "            if feature in tokens:\n",
    "                X[i, j]= 1.0\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* with one feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def naive_bayes_one(feature, train, test, col_feature):    \n",
    "    X_train = design_matrix_one(feature, train[col_feature])\n",
    "    X_test = design_matrix_one(feature, test[col_feature])\n",
    "    y_train = train.iloc[:,2].values\n",
    "    y_test = test.iloc[:,2].values\n",
    "\n",
    "    nbc = sklearn.naive_bayes.BernoulliNB()\n",
    "    model = nbc.fit(X_train, np.ravel(y_train))\n",
    "    pred = model.predict(X_test)\n",
    "    error = 1 - sklearn.metrics.accuracy_score(pred, y_test)\n",
    "    print('Classification error on the test dataset - one feat.: ', error.round(3))  \n",
    "    return error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* with **n** features ranked according to cross-entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def naive_bayes_n(n, train, test, rank_features, col_feature):\n",
    "    y_train = train.iloc[:,2].values\n",
    "    y_test = test.iloc[:,2].values\n",
    "    \n",
    "    feat_to_use = rank_features[:n]\n",
    "    #feat_to_use.remove('pre√ßos')\n",
    "    \n",
    "    X_train = design_matrix(feat_to_use, train[col_feature])\n",
    "    X_test = design_matrix(feat_to_use, test[col_feature])\n",
    "\n",
    "    nbc = sklearn.naive_bayes.BernoulliNB()\n",
    "    model = nbc.fit(X_train, np.ravel(y_train))\n",
    "    y_pred = model.predict(X_test)\n",
    "    error  = 1 - sklearn.metrics.accuracy_score(y_test, y_pred)\n",
    "    print('Classification error on the test dataset - 10 feat.: ', error.round(3)) \n",
    "    return error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* model selection for the number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def naive_bayes_selection(train, test, rank_features, col_feature):\n",
    "    y_train = train.iloc[:,2].values\n",
    "    y_test = test.iloc[:,2].values\n",
    "    \n",
    "    test_errors = []\n",
    "    cv_errors = []\n",
    "\n",
    "    n_features = np.arange(0, len(rank_features)+1, 10)\n",
    "    n_features[0] = 1 # the first model has 1 feature, then 20, 40, etc\n",
    "\n",
    "    for n in tqdm(n_features):\n",
    "        X_train = design_matrix(rank_features[:n], train[col_feature])\n",
    "        X_test = design_matrix(rank_features[:n], test[col_feature])\n",
    "        nbc = sklearn.naive_bayes.BernoulliNB()\n",
    "        model = nbc.fit(X_train, np.ravel(y_train))\n",
    "\n",
    "        scores = sklearn.model_selection.cross_val_score(model, X_train, y_train, cv=10, scoring = 'accuracy')\n",
    "        cv_errors.append(1-np.mean(scores))\n",
    "\n",
    "        y_pred = model.predict(X_test)\n",
    "        test_errors.append(1 - sklearn.metrics.accuracy_score(y_test, y_pred)) \n",
    "    \n",
    "    return n_features, cv_errors, test_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot classification error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_error(n_features, cv_errors, test_errors, classifier):\n",
    "    fig, ax= plt.subplots(figsize=(7.5,5))\n",
    "    ax.plot(n_features, test_errors, color='#1F77B4', label='Test error')\n",
    "    ax.plot(n_features, cv_errors, color='#FF7F0E', label='CV error')\n",
    "    ax.set_xlabel('Number of features')\n",
    "    ax.set_ylabel('Misclassification rate')\n",
    "    ax.set_ylim([0,1])\n",
    "    plt.title('Misclassification rate for %s' %(classifier))\n",
    "    plt.legend()\n",
    "    sns.despine()\n",
    "    plt.show()\n",
    "\n",
    "    print('Lowest CV error: K = {}'.format(n_features[np.argmin(cv_errors)])) \n",
    "    print('CV error with %s features:' %str(n_features[np.argmin(cv_errors)]), round(min(cv_errors),3))   \n",
    "    print('Test error in this selected model = {:.3f}'.format(test_errors[np.argmin(cv_errors)])) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_evaluation(n, train, test, rank_features, col_feature, cat):\n",
    "    X_train = design_matrix(rank_features[:n], train[col_feature])\n",
    "    X_test = design_matrix(rank_features[:n], test[col_feature])\n",
    "    y_train = train.iloc[:,2].values\n",
    "    y_test = test.iloc[:,2].values\n",
    "    \n",
    "    nbc = sklearn.naive_bayes.BernoulliNB()\n",
    "    model = nbc.fit(X_train, np.ravel(y_train))\n",
    "    y_pred = model.predict(X_test)\n",
    "    prob = model.predict_proba(X_test)\n",
    "    \n",
    "    confusion  = sklearn.metrics.confusion_matrix(y_test, y_pred)\n",
    "    df_cm = pd.DataFrame(confusion, index = [i for i in range(1,cat+1)],\n",
    "                      columns = [i for i in range(1,cat+1)])\n",
    "    plt.figure(figsize = (5,3))\n",
    "    sns.heatmap(df_cm, annot=True, fmt=\"d\");\n",
    "    plt.title('Confusion matrix for multiclass NB')\n",
    "    plt.xlabel('Naive Bayes classifier')\n",
    "    plt.ylabel('Real label')\n",
    "    \n",
    "    df_relative = pd.DataFrame(confusion, index = [i for i in range(1,cat+1)],\n",
    "                  columns = [i for i in range(1,cat+1)])\n",
    "    for row in range(0,cat):\n",
    "        agg = sum(df_cm.iloc[row,:])\n",
    "        for col in range(0,cat):\n",
    "            df_relative.iloc[row, col] = df_cm.iloc[row, col]/agg  \n",
    "            \n",
    "    plt.figure(figsize = (5,3))\n",
    "    sns.heatmap(df_relative, annot=True);\n",
    "    plt.title('Confusion matrix for multiclass NB (%)')\n",
    "    plt.xlabel('Naive Bayes classifier')\n",
    "    plt.ylabel('Real label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All the steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def naive_bayes_test(data, frac, col_feature, features, cat):\n",
    "    train, test = split_train_test(data, frac)\n",
    "\n",
    "    y_train = train.iloc[:,2].values\n",
    "    losses=[]\n",
    "    for feature in features.index:\n",
    "        losses.append(training_error(feature, train, y_train, col_feature))\n",
    "\n",
    "    ranked = pd.Series(losses, index=features.index)\n",
    "    ranked = ranked.sort_values()\n",
    "    rank_features = list(ranked.index) \n",
    "\n",
    "    error_one = naive_bayes_one(ranked.index[0], train, test, col_feature)\n",
    "\n",
    "    error_10 = naive_bayes_n(10, train, test, rank_features, col_feature)\n",
    "\n",
    "    n_features, cv_errors, test_errors = naive_bayes_selection(train, test, rank_features, col_feature)\n",
    "    plot_error(n_features, cv_errors, test_errors)\n",
    "    \n",
    "    model_evaluation(n_features[np.argmin(cv_errors)], train, test, rank_features, col_feature, cat)\n",
    "    \n",
    "    return train, test, ranked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Naive Bayes Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def NB_selection(train, test, rank_features, col_feature, col_label, list_min, list_type):\n",
    "    y_train = train[col_label].values\n",
    "    y_test = test[col_label].values\n",
    "    \n",
    "    test_errors = []\n",
    "    cv_errors = []\n",
    "\n",
    "    n_features = np.arange(0, len(rank_features)+1, 10) # the first model has 1 feature, then 10, 20, etc\n",
    "    n_features[0] = 1 \n",
    "\n",
    "    train_min_feat = train[list_min+list_type].values\n",
    "    test_min_feat = test[list_min+list_type].values\n",
    "    \n",
    "    for n in tqdm(n_features):\n",
    "        \n",
    "        train_word_feat = design_matrix(rank_features[:n], train[col_feature])\n",
    "        X_train = scipy.sparse.hstack((train_min_feat, train_word_feat))\n",
    "                \n",
    "        test_word_feat = design_matrix(rank_features[:n], test[col_feature])\n",
    "        X_test = hstack((test_min_feat, test_word_feat))\n",
    "        \n",
    "        NB = BernoulliNB(alpha = 0.01)\n",
    "        model = NB.fit(X_train, np.ravel(y_train))\n",
    "\n",
    "        scores = sklearn.model_selection.cross_val_score(model, X_train, y_train, cv=10, scoring = 'accuracy')\n",
    "        cv_errors.append(1-np.mean(scores))\n",
    "\n",
    "        y_pred = model.predict(X_test)\n",
    "        test_errors.append(1 - sklearn.metrics.accuracy_score(y_test, y_pred)) \n",
    "    \n",
    "    return n_features, cv_errors, test_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def SVM_selection(train, test, rank_features, col_feature, col_label, list_min, list_type):\n",
    "    y_train = train[col_label].values\n",
    "    y_test = test[col_label].values\n",
    "    \n",
    "    test_errors = []\n",
    "    cv_errors = []\n",
    "\n",
    "    n_features = np.arange(0, len(rank_features)+1, 10) # the first model has 1 feature, then 10, 20, etc\n",
    "    n_features[0] = 1 \n",
    "\n",
    "    train_min_feat = train[list_min+list_type].values\n",
    "    test_min_feat = test[list_min+list_type].values\n",
    "    \n",
    "    for n in tqdm(n_features):\n",
    "        \n",
    "        train_word_feat = design_matrix(rank_features[:n], train[col_feature])\n",
    "        X_train = scipy.sparse.hstack((train_min_feat, train_word_feat))\n",
    "                \n",
    "        test_word_feat = design_matrix(rank_features[:n], test[col_feature])\n",
    "        X_test = hstack((test_min_feat, test_word_feat))\n",
    "        \n",
    "        svm = SGDClassifier(loss='hinge', penalty='l2', alpha=0.001, max_iter=5, random_state=5)\n",
    "        model = svm.fit(X_train, np.ravel(y_train))\n",
    "\n",
    "        scores = sklearn.model_selection.cross_val_score(model, X_train, y_train, cv=10, scoring = 'accuracy')\n",
    "        cv_errors.append(1-np.mean(scores))\n",
    "\n",
    "        y_pred = model.predict(X_test)\n",
    "        test_errors.append(1 - sklearn.metrics.accuracy_score(y_test, y_pred)) \n",
    "    \n",
    "    return n_features, cv_errors, test_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LR_selection(train, test, rank_features, col_feature, col_label, list_min, list_type):\n",
    "    y_train = train[col_label].values\n",
    "    y_test = test[col_label].values\n",
    "    \n",
    "    test_errors = []\n",
    "    cv_errors = []\n",
    "\n",
    "    n_features = np.arange(0, len(rank_features)+1, 10) # the first model has 1 feature, then 10, 20, etc\n",
    "    n_features[0] = 1 \n",
    "\n",
    "    train_min_feat = train[list_min+list_type].values\n",
    "    test_min_feat = test[list_min+list_type].values\n",
    "    \n",
    "    for n in tqdm(n_features):\n",
    "        \n",
    "        train_word_feat = design_matrix(rank_features[:n], train[col_feature])\n",
    "        X_train = scipy.sparse.hstack((train_min_feat, train_word_feat))\n",
    "                \n",
    "        test_word_feat = design_matrix(rank_features[:n], test[col_feature])\n",
    "        X_test = hstack((test_min_feat, test_word_feat))\n",
    "        \n",
    "        logistic = LogisticRegression(penalty = 'l2', C=3, class_weight=\"balanced\")\n",
    "        model = logistic.fit(X_train, np.ravel(y_train))\n",
    "\n",
    "        scores = sklearn.model_selection.cross_val_score(model, X_train, y_train, cv=10, scoring = 'accuracy')\n",
    "        cv_errors.append(1-np.mean(scores))\n",
    "\n",
    "        y_pred = model.predict(X_test)\n",
    "        test_errors.append(1 - sklearn.metrics.accuracy_score(y_test, y_pred)) \n",
    "    \n",
    "    return n_features, cv_errors, test_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_confusion(y_test, y_pred, cat, classifier, palette):\n",
    "\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    \n",
    "    confusion  = sklearn.metrics.confusion_matrix(y_test, y_pred)\n",
    "    df_cm = pd.DataFrame(confusion, index = [i for i in range(1,cat+1)],\n",
    "                      columns = [i for i in range(1,cat+1)])\n",
    "    plt.figure(figsize = (6,4))\n",
    "    sns.heatmap(df_cm, annot=True, fmt=\"d\", cmap=palette);\n",
    "    plt.title('Confusion matrix for %s classifier' %(classifier))\n",
    "    plt.xlabel('%s classifier' %(classifier))\n",
    "    plt.ylabel('Real label')\n",
    "    if cat==10:\n",
    "        tick_marks = ['FL', 'DR+SF', 'FD', 'IM', 'MM', 'IF', 'OB', 'RH', 'DOC', 'INFO+RE' ]\n",
    "    else:\n",
    "        tick_marks = ['other', 'FL+DR+SF']\n",
    "    plt.xticks(np.arange(cat)+0.5, tick_marks, rotation=90)\n",
    "    plt.yticks(np.arange(cat)+0.5, tick_marks[::-1], rotation=0)\n",
    "\n",
    "    \n",
    "    df_relative = pd.DataFrame(confusion, index = [i for i in range(1,cat+1)],\n",
    "                  columns = [i for i in range(1,cat+1)])\n",
    "    for row in range(0,cat):\n",
    "        agg = sum(df_cm.iloc[row,:])\n",
    "        for col in range(0,cat):\n",
    "            df_relative.iloc[row, col] = df_cm.iloc[row, col]/agg  \n",
    "\n",
    "    plt.figure(figsize = (6,4))\n",
    "    sns.heatmap(df_relative, annot=True, fmt='.0%', cmap=palette);\n",
    "    plt.title('Recall confusion matrix for %s' %(classifier))\n",
    "    plt.xlabel('%s classifier' %(classifier))\n",
    "    plt.ylabel('Real label')\n",
    "    if cat==10:\n",
    "        tick_marks = ['FL', 'DR+SF', 'FD', 'IM', 'MM', 'IF', 'OB', 'RH', 'DOC', 'INFO+RE' ]\n",
    "    else:\n",
    "        tick_marks = ['other', 'FL+DR+SF']\n",
    "    plt.xticks(np.arange(cat)+0.5, tick_marks, rotation=90)\n",
    "    plt.yticks(np.arange(cat)+0.5, tick_marks[::-1], rotation=0) \n",
    "    \n",
    "    \n",
    "    df_relative_2 = pd.DataFrame(confusion, index = [i for i in range(1,cat+1)],\n",
    "                  columns = [i for i in range(1,cat+1)])\n",
    "    for col in range(0,cat):\n",
    "        agg = sum(df_cm.iloc[:,col])\n",
    "        for row in range(0,cat):\n",
    "            df_relative_2.iloc[row, col] = df_cm.iloc[row, col]/agg  \n",
    "    \n",
    "    plt.figure(figsize = (6,4))\n",
    "    sns.heatmap(df_relative_2, annot=True, fmt='.0%', cmap=palette);\n",
    "    plt.title('Precision confusion matrix for %s' %(classifier))\n",
    "    plt.xlabel('%s classifier' %(classifier))\n",
    "    plt.ylabel('Real label')\n",
    "    if cat==10:\n",
    "        tick_marks = ['FL', 'DR+SF', 'FD', 'IM', 'MM', 'IF', 'OB', 'RH', 'DOC', 'INFO+RE' ]\n",
    "    else:\n",
    "        tick_marks = ['other', 'FL+DR+SF']\n",
    "    plt.xticks(np.arange(cat)+0.5, tick_marks, rotation=90)\n",
    "    plt.yticks(np.arange(cat)+0.5, tick_marks[::-1], rotation=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CV for all agorithms together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feat_selection_all(train, test, rank_features, col_feature, col_label, list_min, list_type):\n",
    "    y_train = train[col_label].values\n",
    "    y_test = test[col_label].values\n",
    "    \n",
    "    test_errors_lr = []\n",
    "    cv_errors_lr = []\n",
    "    test_errors_svm = []\n",
    "    cv_errors_svm = []\n",
    "    test_errors_nb = []\n",
    "    cv_errors_nb = []\n",
    "\n",
    "    n_features = np.arange(0, len(rank_features)+1, 10) # the first model has 1 feature, then 10, 20, etc\n",
    "    n_features[0] = 1 \n",
    "\n",
    "    train_min_feat = train[list_min+list_type].values\n",
    "    test_min_feat = test[list_min+list_type].values\n",
    "    \n",
    "    for n in tqdm(n_features):\n",
    "        \n",
    "        train_word_feat = design_matrix(rank_features[:n], train[col_feature])\n",
    "        X_train = scipy.sparse.hstack((train_min_feat, train_word_feat))\n",
    "                \n",
    "        test_word_feat = design_matrix(rank_features[:n], test[col_feature])\n",
    "        X_test = hstack((test_min_feat, test_word_feat))\n",
    "        \n",
    "        logistic = LogisticRegression(penalty = 'l2', C=3)\n",
    "        model_lr = logistic.fit(X_train, np.ravel(y_train))\n",
    "\n",
    "        scores_lr = sklearn.model_selection.cross_val_score(model_lr, X_train, y_train, cv=10, scoring = 'accuracy')\n",
    "        cv_errors_lr.append(1-np.mean(scores_lr))\n",
    "\n",
    "        y_pred_lr = model_lr.predict(X_test)\n",
    "        test_errors_lr.append(1 - sklearn.metrics.accuracy_score(y_test, y_pred_lr)) \n",
    "        \n",
    "        svm = SGDClassifier(loss='hinge', penalty='l2', alpha=0.001, max_iter=5, random_state=5)\n",
    "        model_svm = svm.fit(X_train, np.ravel(y_train))\n",
    "\n",
    "        scores_svm = sklearn.model_selection.cross_val_score(model_svm, X_train, y_train, cv=10, scoring = 'accuracy')\n",
    "        cv_errors_svm.append(1-np.mean(scores_svm))\n",
    "\n",
    "        y_pred_svm = model_svm.predict(X_test)\n",
    "        test_errors_svm.append(1 - sklearn.metrics.accuracy_score(y_test, y_pred_svm))\n",
    "        \n",
    "        NB = BernoulliNB(alpha = 0.01)\n",
    "        model_nb = NB.fit(X_train, np.ravel(y_train))\n",
    "\n",
    "        scores_nb = sklearn.model_selection.cross_val_score(model_nb, X_train, y_train, cv=10, scoring = 'accuracy')\n",
    "        cv_errors_nb.append(1-np.mean(scores_nb))\n",
    "\n",
    "        y_pred_nb = model_nb.predict(X_test)\n",
    "        test_errors_nb.append(1 - sklearn.metrics.accuracy_score(y_test, y_pred_nb)) \n",
    "    \n",
    "    return n_features, cv_errors_lr, test_errors_lr, cv_errors_svm, test_errors_svm, cv_errors_nb, test_errors_nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class MinistryDummies(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Takes in dataframe, extracts road name column, outputs average word length\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def transform(self, df, y=None):\n",
    "        \"\"\"The workhorse of this feature extractor\"\"\"\n",
    "        return pd.get_dummies(df['Orgao_Sup_cod'], drop_first=True)\n",
    "\n",
    "    def fit(self, df, y=None):\n",
    "        \"\"\"Returns `self` unless something different happens in train and test\"\"\"\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "525px",
    "left": "29px",
    "top": "108px",
    "width": "334px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
